{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration des méthodes d'estimation des scores pour les athlètes n'ayant pas participé à la compétition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy.matlib import repmat\n",
    "import pandas as pd\n",
    "\n",
    "from points_methods.tensorial_product_estimator import estimate_tensorial_product\n",
    "from points_methods.bpca import bpca_fill\n",
    "from data_handling.database_service import DatabaseService\n",
    "from points_methods.skill_based_method import get_competitors_from_cursor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation de données de courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_data(\n",
    "    nb_competitors, nb_competitions, nb_competitors_per_competition, noise_std=3\n",
    "):\n",
    "    competitor_perfs = np.random.normal(100, 10, nb_competitors)\n",
    "    competition_length = np.random.normal(1, 0.1, nb_competitions)\n",
    "    scores = competition_length[:, np.newaxis] @ competitor_perfs[\n",
    "        np.newaxis, :\n",
    "    ] + noise_std * randn(nb_competitions, nb_competitors)\n",
    "    print(\n",
    "        f\"Erreur moyenne due au bruit lors de la génération des données : {abs(scores - competition_length[:, np.newaxis] @ competitor_perfs[np.newaxis, :]).mean()}\"\n",
    "    )\n",
    "    mask = np.zeros_like(scores)\n",
    "    for i in range(nb_competitions):\n",
    "        mask[\n",
    "            i, random.sample(range(nb_competitors), nb_competitors_per_competition)\n",
    "        ] = 1\n",
    "    return scores, mask, competitor_perfs, competition_length\n",
    "\n",
    "\n",
    "def make_fake_data2(\n",
    "    nb_competitors,\n",
    "    nb_competitions,\n",
    "    nb_competitors_per_competition,\n",
    "    noise_std=3,\n",
    "    category_coef_std=0.03,\n",
    "    category_coefs=[1, 1.05, 1.13, 1.2],\n",
    "    category_part=[0.53, 0.23, 0.16, 0.08],\n",
    "    verbose=False,\n",
    "):\n",
    "    competitor_perfs = np.random.normal(100, 10, nb_competitors)\n",
    "    boundaries = [0] + [int(nb_competitors * part) for part in np.cumsum(category_part)]\n",
    "    competition_length = np.random.normal(1, 0.1, nb_competitions)\n",
    "    scores = np.zeros((nb_competitions, nb_competitors))\n",
    "    applied_coefs = (\n",
    "        np.array(category_coefs)[np.newaxis, :]\n",
    "        + np.random.randn(nb_competitions, len(category_coefs)) * category_coef_std\n",
    "    )\n",
    "    for i in range(nb_competitions):\n",
    "        for j in range(len(category_coefs)):\n",
    "            scores[i, boundaries[j] : boundaries[j + 1]] = (\n",
    "                applied_coefs[i, j]\n",
    "                * competition_length[i]\n",
    "                * competitor_perfs[boundaries[j] : boundaries[j + 1]]\n",
    "            )\n",
    "    scores_with_noise = scores + noise_std * randn(nb_competitions, nb_competitors)\n",
    "    scores_no_noise = scores\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Erreur moyenne due au bruit lors de la génération des données : {abs(scores - scores_with_noise).mean()}\"\n",
    "        )\n",
    "    mask = np.zeros_like(scores)\n",
    "    for i in range(nb_competitions):\n",
    "        mask[\n",
    "            i, random.sample(range(nb_competitors), nb_competitors_per_competition)\n",
    "        ] = 1\n",
    "    return scores_with_noise, mask, competitor_perfs, competition_length, scores_no_noise, applied_coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne due au bruit lors de la génération des données : 2.5522564467497446\n",
      "mu = [ 86.743991   130.8143107   96.98355928 117.74121436 104.62576778\n",
      "  95.38920557  88.13345499  99.29917326 106.35231464  72.31162396]\n",
      "\n",
      "lambda = [1.10393297 1.03859142 0.90858148 1.17503294 0.9970659 ]\n",
      "\n",
      "Number of competitions per competitor : [4. 4. 1. 1. 5. 3. 4. 4. 1. 3.]\n",
      "\n",
      "scores = \n",
      "[[ 95.13060696 149.8942189  111.47177274 133.56988487 117.93945547\n",
      "  103.70093232  94.16813052 109.29871214 111.40537881  85.96820849]\n",
      " [ 88.31543157 137.81053437  96.37933425 117.37571882 111.67030849\n",
      "   97.6912931   89.74979685 103.23685082 110.84197142  76.34662016]\n",
      " [ 81.35004275 115.13915332  86.16763445 106.65914525  93.7804172\n",
      "   95.04106828  83.73719036  90.98514355  95.15771104  66.76297312]\n",
      " [103.53013905 157.00774103 112.75520909 135.38494369 124.25221024\n",
      "  115.24029281 105.26120131 121.10370202 126.1046379   83.51712509]\n",
      " [ 92.35249083 134.14729694  99.6126872  116.93961163 103.81619248\n",
      "   89.27355601  86.56171566  97.8165764  106.42647622  75.52226379]]\n",
      "\n",
      "mask = \n",
      "[[1. 1. 0. 0. 1. 0. 1. 1. 0. 1.]\n",
      " [1. 1. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1. 1. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s, m, mu, _lambda = make_fake_data(10, 5, 6, noise_std=3)\n",
    "print(f\"mu = {mu}\\n\")\n",
    "print(f\"lambda = {_lambda}\\n\")\n",
    "print(f\"Number of competitions per competitor : {np.sum(m, axis=0)}\\n\")\n",
    "print(f\"scores = \\n{s}\\n\")\n",
    "print(f\"mask = \\n{m}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne due au bruit lors de la génération des données : 1.6191904576781875\n",
      "mu = [ 90.21492419  91.67963743  96.81319464  82.57296873  87.24373442\n",
      " 108.48316502 111.40716091  95.30862411 101.21983405  98.58167761]\n",
      "\n",
      "lambda = [0.77590993 0.86151811 0.88153983 1.02450488 0.95946145]\n",
      "\n",
      "Number of competitions per competitor : [3. 3. 3. 3. 4. 4. 3. 3. 3. 1.]\n",
      "\n",
      "scores = \n",
      "[[ 69.56345949  67.50879978  74.85239614  62.0449582   68.52255169\n",
      "   88.04619347  90.18236556  81.30608447  85.67196282  92.819453  ]\n",
      " [ 77.74797089  81.45816894  84.68324463  73.87088605  78.04811432\n",
      "   93.33399453 100.82698119  91.36154436  99.90609572  99.92535528]\n",
      " [ 75.89941334  81.08323396  79.68463894  69.42010576  76.49834467\n",
      "  101.75735505 106.47211244  95.18887587 100.95441763 106.78443119]\n",
      " [ 89.73778452  97.33466772  99.91492248  83.03858479  91.36245433\n",
      "  117.90016841 116.50076315 106.57931125 112.63563622 120.67402256]\n",
      " [ 87.75877198  82.20307325  91.48007098  80.27686112  84.69396602\n",
      "  108.5637375  110.06123487 106.4582987  113.47348446 116.87022175]]\n",
      "\n",
      "mask = \n",
      "[[1. 0. 1. 0. 1. 1. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 0. 1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 1. 1. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1. 1. 1. 1. 0.]]\n",
      "\n",
      "applied_coefs = \n",
      "[[1.00688898 1.04394298 1.10552066 1.19239472]\n",
      " [1.00026519 1.04481098 1.12094111 1.18496184]\n",
      " [0.97946749 1.05813942 1.14029185 1.21019273]\n",
      " [1.01209129 1.03623828 1.08677981 1.17491601]\n",
      " [0.99471369 1.04869158 1.17955525 1.22291791]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores, mask, competitor_perfs, competition_length, scores_no_noise, applied_coefs = make_fake_data2(10, 5, 6, noise_std=2, category_coef_std=0.02)\n",
    "\n",
    "print(f\"mu = {competitor_perfs}\\n\")\n",
    "print(f\"lambda = {competition_length}\\n\")\n",
    "print(f\"Number of competitions per competitor : {np.sum(mask, axis=0)}\\n\")\n",
    "print(f\"scores = \\n{scores}\\n\")\n",
    "print(f\"mask = \\n{mask}\\n\")\n",
    "print(f\"applied_coefs = \\n{applied_coefs}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération de données réelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_table(starting_date):\n",
    "    phase = None\n",
    "    db_service = DatabaseService()\n",
    "    ending_date = starting_date.replace(year=starting_date.year + 1)\n",
    "    db_service.get_competition_dates(starting_date)\n",
    "    competitors_cursor = db_service.get_competitors_on_period(\n",
    "        starting_date, ending_date, phase=phase\n",
    "    )\n",
    "    competitors, nb_participations = get_competitors_from_cursor(competitors_cursor)\n",
    "    competitions = list(\n",
    "        db_service.get_competitions_on_period(\n",
    "            starting_date, ending_date, phase=phase\n",
    "        )\n",
    "    )\n",
    "    scores_df = pd.DataFrame(0, columns=competitors, index=competitions, dtype=float)\n",
    "    participations_df = pd.DataFrame(0, columns=competitors, index=competitions)\n",
    "    participations = db_service.get_participations_on_period(\n",
    "        starting_date, ending_date, phase=phase\n",
    "    )\n",
    "    for participation in participations:\n",
    "        competitor = (\n",
    "            participation[\"competitorName\"],\n",
    "            participation[\"competitorCategory\"],\n",
    "        )\n",
    "        competition_name = participation[\"competitionName\"]\n",
    "        score = participation[\"score\"]\n",
    "        scores_df.at[competition_name, competitor] = score\n",
    "        participations_df.at[competition_name, competitor] = 1\n",
    "    return scores_df, participations_df\n",
    "\n",
    "def filter_score_table(s, m):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, m = get_scores_table(datetime(2019, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de compétitions : 246\n",
      "nombre de compétiteurs : 3599\n",
      "nombre moyen de participants par compétition : 138.369918699187\n"
     ]
    }
   ],
   "source": [
    "print(f\"nombre de compétitions : {m.shape[0]}\")\n",
    "print(f\"nombre de compétiteurs : {m.shape[1]}\")\n",
    "print(f\"nombre moyen de participants par compétition : {m.sum(axis=1).mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation des scores par itérations de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_pca_score_estimation(\n",
    "    s_masked,\n",
    "    m,\n",
    "    n_components=1,\n",
    "    nb_iter=100,\n",
    "    initial_noise_std=10,\n",
    "    noise_std=0,\n",
    "    verbose=False,\n",
    "    initialisation=\"tensorial_product_estimation\",\n",
    "    s=None\n",
    "):\n",
    "    # s_masked = s_masked + (\n",
    "    #     (s_masked.sum() / m.sum())\n",
    "    #     + np.random.randn(*s_masked.shape) * initial_noise_std\n",
    "    # ) * (1 - m)\n",
    "    #s_masked = s_masked + (s_masked.sum() / m.sum()) * (1 - m)\n",
    "    if initialisation == \"tensorial_product_estimation\":\n",
    "        competition_vector, competitor_vector = estimate_tensorial_product(\n",
    "            pd.DataFrame(s * m), participations_df=pd.DataFrame(m), removing_rate=0\n",
    "        )\n",
    "        s_estimated = competition_vector.values[:, np.newaxis] * competitor_vector.values[np.newaxis, :]\n",
    "        s_masked = s_estimated * (1 - m) + s_masked * m\n",
    "    elif initialisation == \"random\":\n",
    "        s_masked = s_masked + np.random.randn(*s_masked.shape) * initial_noise_std * (1 - m)\n",
    "    elif initialisation == \"zeros\":\n",
    "        pass # s_masked = s_masked * m\n",
    "    for i in range(nb_iter):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        y = pca.fit_transform(s_masked)\n",
    "        recons_s = pca.inverse_transform(y)\n",
    "        new_s_masked = (\n",
    "            recons_s * (1 - m) + s_masked * m + np.random.randn(*s_masked.shape) * noise_std * (1 - m)\n",
    "        )\n",
    "        old_s_masked = s_masked\n",
    "        s_masked = new_s_masked\n",
    "        if i % max(nb_iter//10, 1) == 0 and verbose:\n",
    "            print(\n",
    "                f\"\\niteration {i}, écart absolu moyen sur la reconstruction des données connues = {(abs(pca.inverse_transform(y) - s_masked) * m).sum()/m.sum()}\"\n",
    "            )\n",
    "            if s is not None:\n",
    "                print(f\"iteration {i}, écart absolu moyen = {abs(s_masked-s).mean()}\")\n",
    "                print(f\"iteration {i}, écart absolu moyen sur les données inconnues = {(abs(s_masked - s) * (1-m)).sum()/(1-m).sum()}\")\n",
    "                print(f\"itération {i}, variance de la projection : {pca.explained_variance_ratio_}\")\n",
    "                print(f\"itération {i}, modification moyenne des données inconnues {(abs(s_masked - old_s_masked)*(1-m)).sum()/(1-m).sum()}\")\n",
    "    recons_s = pca.inverse_transform(y)\n",
    "    new_s_masked = recons_s * (1 - m) + s_masked * m\n",
    "    return s_masked\n",
    "\n",
    "def iter_tpca_score_estimation(\n",
    "    s_masked,\n",
    "    m,\n",
    "    n_components=1,\n",
    "    nb_iter=100,\n",
    "    initial_noise_std=10,\n",
    "    noise_std=0,\n",
    "    verbose=False,\n",
    "    initialisation=\"tensorial_product_estimation\",\n",
    "    s=None\n",
    "):\n",
    "    if initialisation == \"tensorial_product_estimation\":\n",
    "        competition_vector, competitor_vector = estimate_tensorial_product(\n",
    "            pd.DataFrame(s * m), participations_df=pd.DataFrame(m), removing_rate=0\n",
    "        )\n",
    "        s_estimated = competition_vector.values[:, np.newaxis] * competitor_vector.values[np.newaxis, :]\n",
    "        s_masked = s_estimated * (1 - m) + s_masked * m\n",
    "    elif initialisation == \"random\":\n",
    "        s_masked = s_masked + np.random.randn(*s_masked.shape) * initial_noise_std * (1 - m) + (s_masked.sum() / m.sum()) * (1 - m)\n",
    "    elif initialisation == \"zeros\":\n",
    "        pass # s_masked = s_masked * m\n",
    "    s_masked = s_masked.T\n",
    "    m = m.T\n",
    "    if s is not None:\n",
    "        s = s.T\n",
    "    for i in range(nb_iter):\n",
    "        pca = PCA(n_components=n_components)\n",
    "        y = pca.fit_transform(s_masked)\n",
    "        recons_s = pca.inverse_transform(y)\n",
    "        new_s_masked = (\n",
    "            recons_s * (1 - m) + s_masked * m + np.random.randn(*s_masked.shape) * noise_std * (1 - m)\n",
    "        )\n",
    "        old_s_masked = s_masked\n",
    "        s_masked = new_s_masked\n",
    "        if i % max(nb_iter//10, 1) == 0 and verbose:\n",
    "            print(\n",
    "                f\"\\niteration {i}, écart absolu moyen sur la reconstruction des données connues = {(abs(pca.inverse_transform(y) - s_masked) * m).sum()/m.sum()}\"\n",
    "            )\n",
    "            if s is not None:\n",
    "                print(f\"iteration {i}, écart absolu moyen = {abs(s_masked-s).mean()}\")\n",
    "                print(f\"iteration {i}, écart absolu moyen sur les données inconnues = {(abs(s_masked - s) * (1-m)).sum()/(1-m).sum()}\")\n",
    "                print(f\"itération {i}, variance de la projection : {pca.explained_variance_ratio_}\")\n",
    "                print(f\"itération {i}, modification moyenne des données inconnues {(abs(s_masked - old_s_masked)*(1-m)).sum()/(1-m).sum()}\")\n",
    "    recons_s = pca.inverse_transform(y)\n",
    "    new_s_masked = recons_s * (1 - m) + s_masked * m\n",
    "    return s_masked.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne due au bruit lors de la génération des données : 3.988898032074559\n",
      "\n",
      "iteration 0, écart absolu moyen sur la reconstruction des données connues = 3.9929726823242193\n",
      "iteration 0, écart absolu moyen = 1.3601007712756072\n",
      "iteration 0, écart absolu moyen sur les données inconnues = 4.080302313826821\n",
      "itération 0, variance de la projection : [8.86279796e-01 7.99208739e-03 4.66593124e-03 2.34488769e-03\n",
      " 5.60764046e-04]\n",
      "itération 0, modification moyenne des données inconnues 1.2999085468625258\n",
      "\n",
      "iteration 1, écart absolu moyen sur la reconstruction des données connues = 3.9387325499669843\n",
      "iteration 1, écart absolu moyen = 1.3482630236030824\n",
      "iteration 1, écart absolu moyen sur les données inconnues = 4.044789070809247\n",
      "itération 1, variance de la projection : [0.88280057 0.01384924 0.00804448 0.00404445 0.0008983 ]\n",
      "itération 1, modification moyenne des données inconnues 0.4517816430561072\n",
      "\n",
      "iteration 2, écart absolu moyen sur la reconstruction des données connues = 3.9312029747744233\n",
      "iteration 2, écart absolu moyen = 1.348221138107092\n",
      "iteration 2, écart absolu moyen sur les données inconnues = 4.044663414321277\n",
      "itération 2, variance de la projection : [0.87959097 0.01617772 0.00938518 0.00472011 0.00103314]\n",
      "itération 2, modification moyenne des données inconnues 0.16994496962668845\n",
      "\n",
      "iteration 3, écart absolu moyen sur la reconstruction des données connues = 3.9298838971405647\n",
      "iteration 3, écart absolu moyen = 1.348842626024288\n",
      "iteration 3, écart absolu moyen sur les données inconnues = 4.046527878072864\n",
      "itération 3, variance de la projection : [0.87818911 0.01705578 0.00989311 0.00497619 0.00109054]\n",
      "itération 3, modification moyenne des données inconnues 0.07164652196184726\n",
      "\n",
      "iteration 4, écart absolu moyen sur la reconstruction des données connues = 3.929533799283978\n",
      "iteration 4, écart absolu moyen = 1.34920732057816\n",
      "iteration 4, écart absolu moyen sur les données inconnues = 4.04762196173448\n",
      "itération 4, variance de la projection : [0.8776227  0.01739271 0.01008942 0.00507492 0.00111733]\n",
      "itération 4, modification moyenne des données inconnues 0.037069272900150045\n",
      "\n",
      "iteration 5, écart absolu moyen sur la reconstruction des données connues = 3.929382322131855\n",
      "iteration 5, écart absolu moyen = 1.3493874923487166\n",
      "iteration 5, écart absolu moyen sur les données inconnues = 4.04816247704615\n",
      "itération 5, variance de la projection : [0.87739161 0.01752674 0.01016798 0.0051142  0.00113132]\n",
      "itération 5, modification moyenne des données inconnues 0.024520308103897694\n",
      "\n",
      "iteration 6, écart absolu moyen sur la reconstruction des données connues = 3.9292796139831467\n",
      "iteration 6, écart absolu moyen = 1.3494780447037498\n",
      "iteration 6, écart absolu moyen sur les données inconnues = 4.0484341341112495\n",
      "itération 6, variance de la projection : [0.8772937  0.01758253 0.01020073 0.00513042 0.00113957]\n",
      "itération 6, modification moyenne des données inconnues 0.01969752570717514\n",
      "\n",
      "iteration 7, écart absolu moyen sur la reconstruction des données connues = 3.929198175089692\n",
      "iteration 7, écart absolu moyen = 1.349524794150426\n",
      "iteration 7, écart absolu moyen sur les données inconnues = 4.048574382451278\n",
      "itération 7, variance de la projection : [0.87724982 0.01760715 0.01021503 0.00513741 0.00114506]\n",
      "itération 7, modification moyenne des données inconnues 0.01708575499500813\n",
      "\n",
      "iteration 8, écart absolu moyen sur la reconstruction des données connues = 3.929131544615194\n",
      "iteration 8, écart absolu moyen = 1.3495533460333482\n",
      "iteration 8, écart absolu moyen sur les données inconnues = 4.048660038100045\n",
      "itération 8, variance de la projection : [0.87722856 0.01761885 0.01022163 0.00514058 0.00114907]\n",
      "itération 8, modification moyenne des données inconnues 0.01567935811715777\n",
      "\n",
      "iteration 9, écart absolu moyen sur la reconstruction des données connues = 3.929075509710862\n",
      "iteration 9, écart absolu moyen = 1.3495757546833613\n",
      "iteration 9, écart absolu moyen sur les données inconnues = 4.0487272640500835\n",
      "itération 9, variance de la projection : [0.87721718 0.01762496 0.01022491 0.00514211 0.00115225]\n",
      "itération 9, modification moyenne des données inconnues 0.014423746393556917\n",
      "\n",
      "Erreur absolue moyenne : 1.3495757546833613\n",
      "Erreur absolue moyenne sur les valeurs reconstruites : 4.0487272640500835\n",
      "Erreur asolue maximale : 24.036856966042777\n",
      "Indices les plus mal reconstruits parmis les données inconnues :\n",
      "[(1937, 15), (362, 12), (2799, 8), (553, 7), (2815, 7), (2477, 6), (2276, 6), (1627, 6), (1469, 6), (2609, 5)]\n",
      "\n",
      "Indices les plus mal reconstruits parmis les données connues :\n",
      "[(1346, 2), (1024, 2), (1985, 2), (2935, 2), (2372, 2), (1116, 2), (2428, 2), (597, 2), (164, 2), (244, 2)]\n",
      "\n",
      "Nombre de competitions pour le compétiteur le plus mal reconstruit : 205.0\n",
      "\n",
      "Nombre de competitions moyen par competiteur : 200.0\n",
      "\n",
      "Erreur max pour le 1ème compétiteur le plus mal reconstruit : 16.521896770602922\n",
      "\n",
      "Erreur sur la valeur min pour la course 0 : 1.7937877796728685\n",
      "Erreur sur la valeur min pour la course 1 : 1.1675731151569124\n",
      "Erreur sur la valeur min pour la course 2 : 5.954933296104045\n",
      "Erreur sur la valeur min pour la course 3 : 6.032193748723955\n",
      "Erreur sur la valeur min pour la course 4 : 0.9045968270261611\n",
      "Erreur sur la valeur min pour la course 5 : 8.323700556908577\n",
      "Erreur sur la valeur min pour la course 6 : 3.8693754940725142\n",
      "Erreur sur la valeur min pour la course 7 : 8.028729044598087\n",
      "Erreur sur la valeur min pour la course 8 : 2.291538528945267\n",
      "Erreur sur la valeur min pour la course 9 : 2.7586628673955857\n",
      "Erreur sur la valeur min pour la course 10 : 5.364623224364543\n",
      "Erreur sur la valeur min pour la course 11 : 1.103990866490605\n",
      "Erreur sur la valeur min pour la course 12 : 6.576132201413472\n",
      "Erreur sur la valeur min pour la course 13 : 2.159399945883308\n",
      "Erreur sur la valeur min pour la course 14 : 3.622303843650485\n",
      "Erreur sur la valeur min pour la course 15 : 4.252264556109033\n",
      "Erreur sur la valeur min pour la course 16 : 0.9340647869289072\n",
      "Erreur sur la valeur min pour la course 17 : 2.8627509004112994\n",
      "Erreur sur la valeur min pour la course 18 : 0.3509212766587524\n",
      "Erreur sur la valeur min pour la course 19 : 1.9361615400990715\n",
      "\n",
      "Erreur maximale de la valeur min sur l'ensemble des courses : 13.013900124779717\n",
      "\n",
      "Erreur moyenne de la valeur min sur l'ensemble des courses : 3.2443062283022033\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "s, m, competitor_perfs, competition_length, s_no_noise, applied_coefs = make_fake_data2(\n",
    "    3000, 300, 2000, noise_std=5, category_coef_std=0.03\n",
    ")\n",
    "# s, m, mu, _lambda = make_fake_data(3000, 300, 200, noise_std=0)\n",
    "# s, m, mu, _lambda = make_fake_data(300, 30, 100, noise_std=0)\n",
    "# s, m, mu, _lambda = make_fake_data(15, 5, 10, noise_std=0)\n",
    "s_masked = s * m\n",
    "s_masked = iter_tpca_score_estimation(\n",
    "    s_masked,\n",
    "    m,\n",
    "    n_components=5,\n",
    "    nb_iter=10,\n",
    "    verbose=True,\n",
    "    s=s,\n",
    "    initialisation=\"tensorial_product_estimation\",\n",
    ")\n",
    "print(f\"\\nErreur absolue moyenne : {abs(s_masked - s).mean()}\")\n",
    "print(\n",
    "    f\"Erreur absolue moyenne sur les valeurs reconstruites : {abs((s_masked - s) * (1-m)).sum()/(1-m).sum()}\"\n",
    ")\n",
    "print(f\"Erreur asolue maximale : {abs(s_masked - s).max()}\")\n",
    "\n",
    "counter_unseen = Counter(np.argmax(abs(s_masked - s_no_noise) * (1 - m), axis=1))\n",
    "print(\n",
    "    f\"Indices les plus mal reconstruits parmis les données inconnues :\\n{counter_unseen.most_common(10)}\"\n",
    ")\n",
    "counter_seen = Counter(np.argmax(abs(s_masked - s_no_noise) * m, axis=1))\n",
    "print(\n",
    "    f\"\\nIndices les plus mal reconstruits parmis les données connues :\\n{counter_seen.most_common(10)}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nNombre de competitions pour le compétiteur le plus mal reconstruit : {np.sum(m[:, counter_seen.most_common(1)[0][0]])}\"\n",
    ")\n",
    "print(f\"\\nNombre de competitions moyen par competiteur : {np.sum(m, axis=0).mean()}\")\n",
    "j = 1\n",
    "print(\n",
    "    f\"\\nErreur max pour le {j}ème compétiteur le plus mal reconstruit : {abs(s_masked - s_no_noise)[:, counter_seen.most_common(j)[j-1][0]].max()}\\n\"\n",
    ")\n",
    "for i in range(20):\n",
    "    print(\n",
    "        f\"Erreur sur la valeur min pour la course {i} : {abs(s_masked[i,:].min() - s_no_noise[i,:].min())}\"\n",
    "    )\n",
    "print(\n",
    "    f\"\\nErreur maximale de la valeur min sur l'ensemble des courses : {abs(s_masked.min(axis=1) - s_no_noise.min(axis=1)).max()}\"\n",
    ")\n",
    "print(\n",
    "    f\"\\nErreur moyenne de la valeur min sur l'ensemble des courses : {abs(s_masked.min(axis=1) - s_no_noise.min(axis=1)).mean()}\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation du Bayesian principal component analysis (bpca)\n",
    "\n",
    "(Temps de calcul trop long pour la taille de données souhaitée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = True\n",
    "if not ignore:\n",
    "    s, m, mu, _lambda = make_fake_data(3000, 300, 200, noise_std=5)\n",
    "    #s, m, mu, _lambda = make_fake_data(300, 30, 100, noise_std=3)\n",
    "    print(m.sum(axis=0).min())\n",
    "    s_masked = s * m\n",
    "    for i in range(s_masked.shape[0]):\n",
    "        for j in range(s_masked.shape[1]):\n",
    "            if m[i, j] == 0:\n",
    "                s_masked[i, j] = np.nan\n",
    "    s_masked = pd.DataFrame(s_masked)\n",
    "    s_filled = bpca_fill(s_masked, max_iter=20, verbose=True)\n",
    "    print(f\"erreur absolue moyenne d'estimation des temps : {abs(s_filled-s).mean().mean()}\")\n",
    "    print(f\"erreur moyenne d'estimation des temps : {(s_filled-s).mean().mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode par estimation des coefficients de competition et des coefficients de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne due au bruit lors de la génération des données : 3.9892826011004394\n",
      "erreur absolue moyenne d'estimation des temps : 4.530270927065136\n",
      "erreur d'estimation des temps : -0.00025303700794469553\n",
      "Erreur absolue moyenne sur les valeurs reconstruites : 4.547384590621052\n",
      "écart type de l'erreur d'estimation des temps : 5.689590798727802\n",
      "erreur absolue moyenne d'estimation des longueurs de course : 0.013925015268163855\n",
      "erreur absolue maximale d'estimation des participations : 26.40526666425481\n",
      "\n",
      "Erreur max pour le 1ème compétiteur le plus mal reconstruit : 5.777238095458557\n",
      "\n",
      "Erreur sur la valeur min pour la course 0 : 0.9986736406732888\n",
      "Erreur sur la valeur min pour la course 1 : 0.25431412697564326\n",
      "Erreur sur la valeur min pour la course 2 : 0.362328111872003\n",
      "Erreur sur la valeur min pour la course 3 : 0.8509954999409501\n",
      "Erreur sur la valeur min pour la course 4 : 0.09062116064083625\n",
      "Erreur sur la valeur min pour la course 5 : 0.688856219317671\n",
      "Erreur sur la valeur min pour la course 6 : 1.6835251688715047\n",
      "Erreur sur la valeur min pour la course 7 : 1.1088978261802254\n",
      "Erreur sur la valeur min pour la course 8 : 0.8161978316871199\n",
      "Erreur sur la valeur min pour la course 9 : 0.7486504626683228\n",
      "Erreur sur la valeur min pour la course 10 : 1.1196368721175958\n",
      "Erreur sur la valeur min pour la course 11 : 0.23272673394450294\n",
      "Erreur sur la valeur min pour la course 12 : 0.4950422168735642\n",
      "Erreur sur la valeur min pour la course 13 : 1.2214191881172667\n",
      "Erreur sur la valeur min pour la course 14 : 1.6073398406063575\n",
      "Erreur sur la valeur min pour la course 15 : 2.360942522068555\n",
      "Erreur sur la valeur min pour la course 16 : 0.2849099568242508\n",
      "Erreur sur la valeur min pour la course 17 : 0.3222732930626364\n",
      "Erreur sur la valeur min pour la course 18 : 1.5740877925861128\n",
      "Erreur sur la valeur min pour la course 19 : 0.45334516341623754\n",
      "\n",
      "Erreur maximale de la valeur min sur l'ensemble des courses : 3.457803391669671\n",
      "\n",
      "Erreur moyenne de la valeur min sur l'ensemble des courses : 0.9173469224196765\n"
     ]
    }
   ],
   "source": [
    "#s, m, mu, _lambda = make_fake_data(300, 30, 100, noise_std=3)\n",
    "#s, m, mu, _lambda = make_fake_data(3000, 300, 200, noise_std=3)\n",
    "s, m, competitor_perfs, competition_length, s_no_noise, applied_coefs = make_fake_data2(3000, 300, 200, noise_std=5, category_coef_std=0.03)\n",
    "competition_vector, competitor_vector = estimate_tensorial_product(\n",
    "    pd.DataFrame(s * m), participations_df=pd.DataFrame(m), removing_rate=0\n",
    ")\n",
    "s_filled = competition_vector.values[:, np.newaxis] * competitor_vector.values[np.newaxis, :]\n",
    "print(f\"erreur absolue moyenne d'estimation des temps : {abs(s_filled-s).mean()}\")\n",
    "print(f\"erreur d'estimation des temps : {(s_filled-s).mean()}\")\n",
    "print(\n",
    "    f\"Erreur absolue moyenne sur les valeurs reconstruites : {abs((s_filled - s) * (1-m)).sum()/(1-m).sum()}\"\n",
    ")\n",
    "print(f\"écart type de l'erreur d'estimation des temps : {(s_filled-s).std()}\")\n",
    "print(f\"erreur absolue moyenne d'estimation des longueurs de course : {abs(competition_vector.values - competition_length).mean()}\")\n",
    "print(f\"erreur absolue maximale d'estimation des participations : {abs(competitor_vector.values - competitor_perfs).max()}\")\n",
    "print(f\"\\nErreur max pour le {j}ème compétiteur le plus mal reconstruit : {abs(s_filled - s_no_noise)[:, counter_seen.most_common(j)[j-1][0]].max()}\\n\")\n",
    "for i in range(20):\n",
    "    print(f\"Erreur sur la valeur min pour la course {i} : {abs(s_filled[i,:].min() - s_no_noise[i,:].min())}\")\n",
    "print(f\"\\nErreur maximale de la valeur min sur l'ensemble des courses : {abs(s_filled.min(axis=1) - s_no_noise.min(axis=1)).max()}\")\n",
    "print(f\"\\nErreur moyenne de la valeur min sur l'ensemble des courses : {abs(s_filled.min(axis=1) - s_no_noise.min(axis=1)).mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de points par estimation du classement parmis l'ensemble des compétiteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pts(s, category_part=[0.53, 0.23, 0.16, 0.08]):\n",
    "    boundaries = [0] + [int(s.shape[1] * part) for part in np.cumsum(category_part)]\n",
    "    pts = np.zeros(s.shape)\n",
    "    for cat in range(len(boundaries)-1):\n",
    "        sub_s = s[:, boundaries[cat]:boundaries[cat+1]]\n",
    "        pts[:, boundaries[cat]:boundaries[cat+1]] = np.argsort(sub_s, axis=1)\n",
    "    return pts\n",
    "\n",
    "def evaluate_pts(s_estimated, s_true, category_part=[0.53, 0.23, 0.16, 0.08], max_ranking=500):\n",
    "    pts_estimated = compute_pts(s_estimated, category_part)\n",
    "    pts_true = compute_pts(s_true, category_part)\n",
    "    pts_estimated[pts_estimated > max_ranking] = max_ranking\n",
    "    pts_true[pts_true > max_ranking] = max_ranking\n",
    "    return abs(pts_estimated-pts_true).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison des 2 méthodes pour l'estimation du rang au classement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores pour la méthode PCA : 139.53579377777777\n",
      "Moyenne des scores pour la méthode Tensorial Product : 139.54827555555556\n"
     ]
    }
   ],
   "source": [
    "pca_scores = list()\n",
    "tprod_scores = list()\n",
    "N_tests = 10\n",
    "for i in range(N_tests):\n",
    "    (\n",
    "        s,\n",
    "        m,\n",
    "        competitor_perfs,\n",
    "        competition_length,\n",
    "        s_no_noise,\n",
    "        applied_coefs,\n",
    "    ) = make_fake_data2(\n",
    "        3000,\n",
    "        300,\n",
    "        200,\n",
    "        noise_std=3,\n",
    "        category_coef_std=0.03,\n",
    "    )\n",
    "    s_masked = s * m\n",
    "\n",
    "\n",
    "    s_estimated_pca = iter_tpca_score_estimation(\n",
    "        s_masked.copy(),\n",
    "        m,\n",
    "        n_components=5,\n",
    "        nb_iter=10,\n",
    "        initial_noise_std=0,\n",
    "        noise_std=0.0,\n",
    "        verbose=False,\n",
    "        s=s_no_noise,\n",
    "    )\n",
    "    competition_vector, competitor_vector = estimate_tensorial_product(\n",
    "        pd.DataFrame(s_masked.copy()), participations_df=pd.DataFrame(m), removing_rate=0\n",
    "    )\n",
    "    s_estimated_tprod = (\n",
    "        competition_vector.values[:, np.newaxis] * competitor_vector.values[np.newaxis, :]\n",
    "    )\n",
    "    pca_eval = evaluate_pts(s_estimated_pca, s)\n",
    "    tprod_eval = evaluate_pts(s_estimated_tprod, s)\n",
    "    pca_scores.append(pca_eval)\n",
    "    tprod_scores.append(tprod_eval)\n",
    "\n",
    "print(f\"Moyenne des scores pour la méthode PCA : {np.mean(pca_scores)}\")\n",
    "print(f\"Moyenne des scores pour la méthode Tensorial Product : {np.mean(tprod_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pts-calc-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
